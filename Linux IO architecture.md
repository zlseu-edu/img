<!-- TOC -->

- [Linux IO 体系层次结构](#linux-io-体系层次结构)
    - [在IO调度层和通用块层实现限速策略](#在io调度层和通用块层实现限速策略)
- [IO调度层](#io调度层)
- [通用块设备层](#通用块设备层)
- [cgroup blkio](#cgroup-blkio)

<!-- /TOC -->

# Linux IO 体系层次结构

![](https://github.com/zlseu-edu/img/blob/master/Linux%20IO%20architecture.png)

 1. VFS: 虚拟文件系统。
 2. 文件系统层: 不同文件系统实现自己的操作。
 3. 页缓存层: 缓存在内存，还没有写入存储设备，必要时由内核第三方进程写回磁盘。
 4. 通用块层: 对接各种不同属性的块设备，对上提供统一的Block IO请求标准。
 5. IO调度层: 针对机械硬盘的各种调度方法就是在这实现的。
 6. 块设备驱动层: 驱动层对外提供相对比较高级的设备操作接口，往往是C语言的，而下层对接设备本身的操作方法和规范。
 7. 块设备层: 具体的物理设备了，定义了各种真对设备操作方法和规范。

## 在IO调度层和通用块层实现限速策略

 - 块设备驱动层、块设备层都是相关具体设备的，如果在这个层次提供，那就不是内核全局的功能，而是某些设备自己的feture。
 - 文件系统层也可以实现，但是如果要全局实现也是不可能的，需要每种文件系统中都实现一遍，成本太高。
 - VFS和page cache这样的机制并不是面向块设备设计的，都是做其他事情用的，虽然也在io体系中，但是并不适合用来做block io的限速。
 - IO调度层和通用块层。IO调度层本身已经有队列了，我们只要在队列里面实现一个限速机制即可，但是在IO调度层实现的限速会因为不同调度算法的侧重点不一样而有很多局限性，从通用块层实现的限速，原则上就可以对几乎所有的块设备进行带宽和iops的限制。

# IO调度层

众所周知，机械硬盘的存储介质是磁介质，并且是盘状，用磁头在盘片上移动进行数据的寻址，这类似播放一张唱片。这种结构的特点是，顺序的数据读写效率比较理想，但是如果一旦对盘片有随机读写，那么大量的时间都会浪费在磁头的移动上，这时候就会导致每次IO的响应时间很长，极大的降低IO的响应速度。磁头在盘片上寻道的操作，类似电梯调度，如果在寻道的过程中，能把路过的相关磁道的数据请求都“顺便”处理掉，那么就可以在比较小影响响应速度的前提下，提高整体IO的吞吐量。

三种调度算法:

 - noop: 空操作调度。
 - cfq(完全公平队列调度): 为每个进程创建一个同步IO调度队列，并默认以时间片和请求数限定的方式分配IO资源，以此保证每个进程的IO资源占用是公平的，cfq还实现了针对进程级别的优先级调度。既然时间片分好了，优先级实现了，那么cfq肯定是实现进程级别的权重比例分配的最好方案。内核就是这么做的，cgroup blkio的权重比例限制就是基于cfq调度器实现的。
 
 - deadline调度（最终期限调度): deadline实现了四个队列，其中两个分别处理正常read和write，按扇区号排序，进行正常io的合并处理以提高吞吐量。因为IO请求可能会集中在某些磁盘位置，这样会导致新来的请求一直被合并，于是可能会有其他磁盘位置的io请求被饿死。于是实现了另外两个处理超时read和write的队列，按请求创建时间排序，如果有超时的请求出现，就放进这两个队列，调度算法保证超时（达到最终期限时间）的队列中的请求会优先被处理，防止请求被饿死。由于deadline的特点，无疑在这里无法区分进程，也就不能实现针对进程的io资源控制。


# 通用块设备层

几个IO概念:

 - 一般IO: 一个正常的文件io，需要经过vfs -> buffer\page cache -> 文件系统 -> 通用块设备层 -> IO调度层 -> 块设备驱动 -> 硬件设备这所有几个层次。其实这就是一般IO。当然，不同的状态可能会有变化，比如一个进程正好open并read一个已经存在于page cache中的数据。

 - Direct IO: VFS之后跳过buffer\page cache层，直接从文件系统层进行操作。那么就意味着，无论读还是写，都不会进行cache。效率低。

 - Sync IO & write-through: 这种方式写的数据要等待存储写入返回才能成功返回，所以跟DIO效率差不多，但是，写的数据仍然是要在cache中写入的。

 - write-back: 将目前在cache中还没写回存储的脏数据写回到存储。这个名词一般指的是一个独立的过程，这个过程不是随着应用的写而发生，这往往是内核自己找个时间来单独操作的。

可以将IO过程中，以是否使用缓冲（缓存）的区别，将IO分成了缓存IO（Buffered IO）和直接IO（Direct io）。

而内核在处理write-back的阶段，由于没有相关page cache中的inode是属于那个cgroup的信息记录，所以所有的page cache的回写只能放到cgroup的root组中进行限制，而不能在其他cgroup中进行限制，因为root组的cgroup一般是不做限制的，所以就相当于目前的cgroup的blkio对buffered IO是没有限速支持的。

而在Sync IO和Direct IO的情况下，由于应用程序写的数据是不经过缓存层的，所以能直接感受到速度被限制，一定要等到整个数据按限制好的速度写完或者读完，才能返回。这就是当前cgroup的blkio限制所能起作用的环境限制。

# cgroup blkio

两种模式:

 - throttle: 限制每个进程能使用的绝对IOPS。
 - weight: 每个进程能使用的IOPS的能力的比例，必须通过CFQ调度器来实现。

blkio要求:

 - 必须走directio, 如果buffered io因为最终写IO的进程不是发起IO的进程，结果会有很大的偏差。
 - 调度器必须是CFQ。